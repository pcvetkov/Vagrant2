Vagrant file

# -*- mode: ruby -*-
# vim: set ft=ruby :
home = ENV['HOME']
ENV["LC_ALL"] = "en_US.UTF-8"

MACHINES = {
  :lvm => {
        :box_name => "centos/7",
        :box_version => "1804.02",
        :ip_addr => '192.168.56.10',
    :disks => {
        :sata1 => {
            :dfile => home + '/VirtualBox VMs/rsata1.vdi',
            :size => 1048,
            :port => 1
        },
        :sata2 => {
            :dfile => home + '/VirtualBox VMs/rsata2.vdi',
            :size => 250, # Megabytes
            :port => 2
        },
        :sata3 => {
            :dfile => home + '/VirtualBox VMs/rsata3.vdi',
            :size => 250, # Megabytes
            :port => 3
        },
        :sata4 => {
            :dfile => home + '/VirtualBox VMs/rsata4.vdi',
            :size => 250,
            :port => 4
        },
        :sata5 => {
            :dfile => home + '/VirtualBox VMs/rsata5.vdi',
            :size => 250,
            :port => 5
        }

    }
  },
}

Vagrant.configure("2") do |config|

    config.vm.box_version = "1804.02"
    MACHINES.each do |boxname, boxconfig|
  
        config.vm.define boxname do |box|
  
            box.vm.box = boxconfig[:box_name]
            box.vm.host_name = boxname.to_s
  
            #box.vm.network "forwarded_port", guest: 3260, host: 3260+offset
  
            box.vm.network "private_network", ip: boxconfig[:ip_addr]
  
            box.vm.provider :virtualbox do |vb|
                    vb.customize ["modifyvm", :id, "--memory", "256"]
                    needsController = false
            boxconfig[:disks].each do |dname, dconf|
                unless File.exist?(dconf[:dfile])
                  vb.customize ['createhd', '--filename', dconf[:dfile], '--variant', 'Fixed', '--size', dconf[:size]]
                                  needsController =  true
                            end
  
            end
                    if needsController == true
                       vb.customize ["storagectl", :id, "--name", "SATA", "--add", "sata" ]
                       boxconfig[:disks].each do |dname, dconf|
                           vb.customize ['storageattach', :id,  '--storagectl', 'SATA', '--port', dconf[:port], '--device', 0, '--type', 'hdd', '--medium', dconf[:dfile]]
                       end
                    end
            end
  
        box.vm.provision "shell", inline: <<-SHELL
            mkdir -p ~root/.ssh
            cp ~vagrant/.ssh/auth* ~root/.ssh
            yum install -y mdadm smartmontools hdparm gdisk
          SHELL
  
        end
    end
  end
  
  
  
  
  
  
  2.  Check disk number and size:
  
  [root@localhost ~]# lshw -short | grep disk
/0/100/1.1/0.0.0    /dev/sda   disk        42GB VBOX HARDDISK
/0/100/d/0          /dev/sdb   disk        1098MB VBOX HARDDISK
/0/100/d/1          /dev/sdc   disk        262MB VBOX HARDDISK
/0/100/d/2          /dev/sdd   disk        262MB VBOX HARDDISK
/0/100/d/3          /dev/sde   disk        262MB VBOX HARDDISK
/0/100/d/0.0.0      /dev/sdf   disk        262MB VBOX HARDDISK


  3.  Install mdadm 
  
         yum -y install mdadm



3. Make zero superblocks

mdadm --zero-superblock --force /dev/sd{b,c,d,e,f}
root@localhost ~]# mdadm --zero-superblock --force /dev/sd{b,c,d,e,f}
mdadm: Unrecognised md component device - /dev/sdb
mdadm: Unrecognised md component device - /dev/sdc
mdadm: Unrecognised md component device - /dev/sdd
mdadm: Unrecognised md component device - /dev/sde
mdadm: Unrecognised md component device - /dev/sdf



4. Create md-device RAID6

root@localhost ~]# mdadm --create --verbose /dev/md0 -l 6 -n 5 /dev/sd{b,c,d,e,f}
mdadm: layout defaults to left-symmetric
mdadm: layout defaults to left-symmetric
mdadm: chunk size defaults to 512K
mdadm: size set to 253952K
mdadm: largest drive (/dev/sdb) exceeds size (253952K) by more than 1%
Continue creating array? y
mdadm: Fail to create md0 when using /sys/module/md_mod/parameters/new_array, fallback to creation via node
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.





5. Check up  meta data

[root@localhost ~]# cat /proc/mdstat 
Personalities : [raid6] [raid5] [raid4] 
md0 : active raid6 sdf[4] sde[3] sdd[2] sdc[1] sdb[0]
      761856 blocks super 1.2 level 6, 512k chunk, algorithm 2 [5/5] [UUUUU]

 
 The size of one chunk is 512 kb,  number of chunks is  5
 
 root@localhost ~]# mdadm -D /dev/md0
/dev/md0:
           Version : 1.2
     Creation Time : Sun Feb  5 09:24:16 2023
        Raid Level : raid6
        Array Size : 761856 (744.00 MiB 780.14 MB)
     Used Dev Size : 253952 (248.00 MiB 260.05 MB)
      Raid Devices : 5
     Total Devices : 5
       Persistence : Superblock is persistent

       Update Time : Sun Feb  5 09:24:19 2023
             State : clean 
    Active Devices : 5
   Working Devices : 5
    Failed Devices : 0
     Spare Devices : 0

            Layout : left-symmetric
        Chunk Size : 512K

Consistency Policy : resync

              Name : localhost.localdomain:0  (local to host localhost.localdomain)
              UUID : 85e4ce99:5cd0e6c3:5c9da617:8ebdf75f
            Events : 17

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc
       2       8       48        2      active sync   /dev/sdd
       3       8       64        3      active sync   /dev/sde
       4       8       80        4      active sync   /dev/sdf

  

6.  Create mdadm.conf


mdadm --detail --scan --verbose 

ARRAY /dev/md0 level=raid6 num-devices=5 metadata=1.2 name=localhost.localdomain:0 UUID=85e4ce99:5cd0e6c3:5c9da617:8ebdf75f
   devices=/dev/sdb,/dev/sdc,/dev/sdd,/dev/sde,/dev/sdf
   
   [root@localhost ~]# mkdir /etc/mdadm
[root@localhost ~]# echo "DEVICE partitions" > /etc/mdadm/mdadm.conf

[root@localhost ~]# mdadm --detail --scan --verbose | awk '/ARRAY/ {print}' >> /etc/mdadm/mdadm.conf 
[root@localhost ~]# cat /etc/mdadm/mdadm.conf 
DEVICE partitions
ARRAY /dev/md0 level=raid6 num-devices=5 metadata=1.2 name=localhost.localdomain:0 UUID=85e4ce99:5cd0e6c3:5c9da617:8ebdf75f



7.  To break/repair  RAID

 7.1  Emulate  broken disk
          mdadm /dev/md0 --fail /dev/sde
          mdadm: set /dev/sde faulty in /dev/md0
          cat /proc/mdstat 
          Personalities : [raid6] [raid5] [raid4] 
          md0 : active raid6 sdf[4] sde[3](F) sdd[2] sdc[1] sdb[0]
          761856 blocks super 1.2 level 6, 512k chunk, algorithm 2 [5/4] [UUU_U]

     
     
       mdadm -D /dev/md0
  /dev/md0:
           Version : 1.2
     Creation Time : Sun Feb  5 09:24:16 2023
        Raid Level : raid6
        Array Size : 761856 (744.00 MiB 780.14 MB)
     Used Dev Size : 253952 (248.00 MiB 260.05 MB)
      Raid Devices : 5
     Total Devices : 5
       Persistence : Superblock is persistent

       Update Time : Sun Feb  5 09:37:43 2023
             State : clean, degraded 
    Active Devices : 4
   Working Devices : 4
    Failed Devices : 1
     Spare Devices : 0

            Layout : left-symmetric
        Chunk Size : 512K

Consistency Policy : resync

              Name : localhost.localdomain:0  (local to host localhost.localdomain)
              UUID : 85e4ce99:5cd0e6c3:5c9da617:8ebdf75f
            Events : 19

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc
       2       8       48        2      active sync   /dev/sdd
       -       0        0        3      removed
       4       8       80        4      active sync   /dev/sdf

       3       8       64        -      faulty   /dev/sde

   7.2       Remove  "failed" disk from array
   
        mdadm /dev/md0 --remove /dev/sde
        
       mdadm: hot removed /dev/sde from /dev/md0
       
   7.3 mdadm /dev/md0 --remove /dev/sde
mdadm: hot removed /dev/sde from /dev/md0


   7.4  get back good disk into array
   
   
   mdadm /dev/md0 --add /dev/sde
   mdadm: added /dev/sde
   cat /proc/mdstat 
    Personalities : [raid6] [raid5] [raid4] 
     md0 : active raid6 sde[5] sdf[4] sdd[2] sdc[1] sdb[0]
      761856 blocks super 1.2 level 6, 512k chunk, algorithm 2 [5/5] [UUUUU]


 8. Create partitions on md0 device
 
    parted -s /dev/md0 mklabel gpt
    
    
    parted /dev/md0 mkpart primary ext4 0% 20%
Information: You may need to update /etc/fstab.


[root@localhost ~]# parted /dev/md0 mkpart primary ext4 20% 40%
Information: You may need to update /etc/fstab.

[root@localhost ~]# parted /dev/md0 mkpart primary ext4 40% 60%           
Information: You may need to update /etc/fstab.

[root@localhost ~]# parted /dev/md0 mkpart primary ext4 60% 80%           
Information: You may need to update /etc/fstab.

[root@localhost ~]# parted /dev/md0 mkpart primary ext4 80% 100%          
Information: You may need to update /etc/fstab.

[root@localhost ~]# fdisk -l /dev/md0          
WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion.

Disk /dev/md0: 780 MB, 780140544 bytes, 1523712 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 524288 bytes / 1572864 bytes
Disk label type: gpt
Disk identifier: 615FDC5E-E42A-45F9-8C7E-F51BB3CB3BE0


#         Start          End    Size  Type            Name
 1         3072       304127    147M  Microsoft basic primary
 2       304128       608255  148.5M  Microsoft basic primary
 3       608256       915455    150M  Microsoft basic primary
 4       915456      1219583  148.5M  Microsoft basic primary
 5      1219584      1520639    147M  Microsoft basic primary



   9. Create & mount file systems on partitions
   
   for i in $(seq 1 5); do sudo mkfs.ext4 /dev/md0p$i; done
mke2fs 1.42.9 (28-Dec-2013)
Filesystem label=
OS type: Linux
Block size=1024 (log=0)
Fragment size=1024 (log=0)
Stride=512 blocks, Stripe width=1536 blocks
37696 inodes, 150528 blocks
7526 blocks (5.00%) reserved for the super user
First data block=1
Maximum filesystem blocks=33816576
19 block groups
8192 blocks per group, 8192 fragments per group
1984 inodes per group
Superblock backups stored on blocks: 
	8193, 24577, 40961, 57345, 73729

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (4096 blocks): done
Writing superblocks and filesystem accounting information: done 

mke2fs 1.42.9 (28-Dec-2013)
Filesystem label=
OS type: Linux
Block size=1024 (log=0)
Fragment size=1024 (log=0)
Stride=512 blocks, Stripe width=1536 blocks
38152 inodes, 152064 blocks
7603 blocks (5.00%) reserved for the super user
First data block=1
Maximum filesystem blocks=33816576
19 block groups
8192 blocks per group, 8192 fragments per group
2008 inodes per group
Superblock backups stored on blocks: 
	8193, 24577, 40961, 57345, 73729

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (4096 blocks): done
Writing superblocks and filesystem accounting information: done 

mke2fs 1.42.9 (28-Dec-2013)
Filesystem label=
OS type: Linux
Block size=1024 (log=0)
Fragment size=1024 (log=0)
Stride=512 blocks, Stripe width=1536 blocks
38456 inodes, 153600 blocks
7680 blocks (5.00%) reserved for the super user
First data block=1
Maximum filesystem blocks=33816576
19 block groups
8192 blocks per group, 8192 fragments per group
2024 inodes per group
Superblock backups stored on blocks: 
	8193, 24577, 40961, 57345, 73729

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (4096 blocks): done
Writing superblocks and filesystem accounting information: done 

mke2fs 1.42.9 (28-Dec-2013)
Filesystem label=
OS type: Linux
Block size=1024 (log=0)
Fragment size=1024 (log=0)
Stride=512 blocks, Stripe width=1536 blocks
38152 inodes, 152064 blocks
7603 blocks (5.00%) reserved for the super user
First data block=1
Maximum filesystem blocks=33816576
19 block groups
8192 blocks per group, 8192 fragments per group
2008 inodes per group
Superblock backups stored on blocks: 
	8193, 24577, 40961, 57345, 73729

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (4096 blocks): done
Writing superblocks and filesystem accounting information: done 

mke2fs 1.42.9 (28-Dec-2013)
Filesystem label=
OS type: Linux
Block size=1024 (log=0)
Fragment size=1024 (log=0)
Stride=512 blocks, Stripe width=1536 blocks
37696 inodes, 150528 blocks
7526 blocks (5.00%) reserved for the super user
First data block=1
Maximum filesystem blocks=33816576
19 block groups
8192 blocks per group, 8192 fragments per group
1984 inodes per group
Superblock backups stored on blocks: 
	8193, 24577, 40961, 57345, 73729

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (4096 blocks): done
Writing superblocks and filesystem accounting information: done 

[root@localhost ~]# mkdir -p /raid/part{1,2,3,4,5}
[root@localhost ~]# for i in $(seq 1 5); do mount /dev/md0p$i /raid/part$i; done
[root@localhost ~]# df -h
Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/VolGroup00-LogVol00   38G  844M   37G   3% /
devtmpfs                         109M     0  109M   0% /dev
tmpfs                            118M     0  118M   0% /dev/shm
tmpfs                            118M  4.6M  114M   4% /run
tmpfs                            118M     0  118M   0% /sys/fs/cgroup
/dev/sda2                       1014M   63M  952M   7% /boot
tmpfs                             24M     0   24M   0% /run/user/1000
tmpfs                             24M     0   24M   0% /run/user/0
/dev/md0p1                       139M  1.6M  127M   2% /raid/part1
/dev/md0p2                       140M  1.6M  128M   2% /raid/part2
/dev/md0p3                       142M  1.6M  130M   2% /raid/part3
/dev/md0p4                       140M  1.6M  128M   2% /raid/part4
/dev/md0p5                       139M  1.6M  127M   2% /raid/part5
   

    
    

  

        
         
